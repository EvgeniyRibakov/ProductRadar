"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ HTML —Ñ–∞–π–ª–µ
–ü–æ–º–æ–≥–∞–µ—Ç –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
"""

import re
from pathlib import Path
from bs4 import BeautifulSoup

def find_elements_in_html(html_file: str, search_terms: list):
    """–ù–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ HTML –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    html_path = Path(html_file)
    if not html_path.exists():
        print(f"‚ùå –§–∞–π–ª {html_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    print(f"üìÑ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {html_file}\n")
    
    with open(html_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    soup = BeautifulSoup(content, 'html.parser')
    
    for term in search_terms:
        print(f"\n{'='*60}")
        print(f"üîç –ü–æ–∏—Å–∫: '{term}'")
        print(f"{'='*60}\n")
        
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É
        text_matches = soup.find_all(string=re.compile(term, re.I))
        if text_matches:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(text_matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É:")
            for i, match in enumerate(text_matches[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                parent = match.parent
                if parent:
                    print(f"\n  {i}. –¢–µ–∫—Å—Ç: '{match.strip()[:100]}'")
                    print(f"     –¢–µ–≥: {parent.name}")
                    print(f"     –ö–ª–∞—Å—Å: {parent.get('class', '–Ω–µ—Ç')}")
                    print(f"     ID: {parent.get('id', '–Ω–µ—Ç')}")
                    print(f"     –†–æ–¥–∏—Ç–µ–ª—å: {parent.parent.name if parent.parent else '–Ω–µ—Ç'}")
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
        class_matches = soup.find_all(class_=re.compile(term, re.I))
        if class_matches:
            print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(class_matches)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–ª–∞—Å—Å–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º '{term}':")
            for i, match in enumerate(class_matches[:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                print(f"\n  {i}. –¢–µ–≥: {match.name}")
                print(f"     –ö–ª–∞—Å—Å: {match.get('class', '–Ω–µ—Ç')}")
                print(f"     ID: {match.get('id', '–Ω–µ—Ç')}")
                print(f"     –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): '{match.get_text()[:100]}'")
        
        # –ü–æ–∏—Å–∫ –ø–æ ID
        id_matches = soup.find_all(id=re.compile(term, re.I))
        if id_matches:
            print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(id_matches)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å ID, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º '{term}':")
            for i, match in enumerate(id_matches[:5], 1):
                print(f"\n  {i}. –¢–µ–≥: {match.name}")
                print(f"     ID: {match.get('id', '–Ω–µ—Ç')}")
                print(f"     –ö–ª–∞—Å—Å: {match.get('class', '–Ω–µ—Ç')}")
                print(f"     –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): '{match.get_text()[:100]}'")

if __name__ == "__main__":
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π HTML —Ñ–∞–π–ª
    html_dir = Path("html_dumps")
    if html_dir.exists():
        html_files = list(html_dir.glob("*.html"))
        if html_files:
            latest_file = max(html_files, key=lambda p: p.stat().st_mtime)
            print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∞–π–ª: {latest_file}\n")
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_terms = [
                "Impression",
                "Script",
                "Hook",
                "Target Audience",
                "Audience",
                "Country",
                "First seen",
                "Data",
                "Script Analysis",
                "Hooks",
                "Scripts"
            ]
            
            find_elements_in_html(str(latest_file), search_terms)
        else:
            print("‚ùå HTML —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ html_dumps/")
    else:
        print("‚ùå –ü–∞–ø–∫–∞ html_dumps/ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")








